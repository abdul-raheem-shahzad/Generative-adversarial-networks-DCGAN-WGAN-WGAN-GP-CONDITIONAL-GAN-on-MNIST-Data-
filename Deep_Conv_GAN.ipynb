{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":438,"status":"ok","timestamp":1720009572969,"user":{"displayName":"Abdul Raheem Shahzad","userId":"03038179588193543641"},"user_tz":-300},"id":"khjKkIm4jnBE"},"outputs":[],"source":["from zipfile import ZipFile"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2234688,"status":"ok","timestamp":1720012096029,"user":{"displayName":"Abdul Raheem Shahzad","userId":"03038179588193543641"},"user_tz":-300},"id":"SqGvv0DNj0k3"},"outputs":[],"source":["with ZipFile('/content/drive/MyDrive/PyTorch/GANS/celeb_dataset.zip', 'r') as zip:\n","    zip.extractall('/content/drive/MyDrive/PyTorch/GANS/celeb_dataset')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7378,"status":"ok","timestamp":1720013295131,"user":{"displayName":"Abdul Raheem Shahzad","userId":"03038179588193543641"},"user_tz":-300},"id":"uEh71dsqr5F6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, channels_img, features_d):\n","        super(Discriminator, self).__init__()\n","        self.disc = nn.Sequential(\n","            nn.Conv2d(\n","                channels_img, features_d, kernel_size=4, stride=2, padding=1\n","            ),\n","            nn.LeakyReLU(0.2),\n","            self._block(features_d, features_d*2,4,2,1),\n","            self._block(features_d*2, features_d*4,4,2,1),\n","            self._block(features_d*4, features_d*8,4,2,1),\n","            nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0),\n","            nn.Sigmoid(),\n","\n","\n","        )\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","\n","        )\n","    def forward(self, x):\n","        return self.disc(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, channel_img, features_g):\n","        super(Generator, self).__init__()\n","        self.net = nn.Sequential(\n","            self._block(z_dim, features_g*16, 4, 1, 0),\n","            self._block(features_g*16, features_g*8, 4, 2, 1),\n","            self._block(features_g*8, features_g*4, 4, 2, 1),\n","            self._block(features_g*4, features_g*2, 4, 2, 1),\n","            nn.ConvTranspose2d(\n","                features_g*2, channel_img, kernel_size=4, stride=2, padding=1,\n","            ),\n","            nn.Tanh(),\n","\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","def initialize_weights(model):\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data,0.0,0.02)\n","\n","def test():\n","    N, in_channels, H, W = 8, 3, 64, 64\n","    z_dim = 100\n","    x = torch.randn((N, in_channels, H, W))\n","    disc = Discriminator(in_channels, 8)\n","    initialize_weights(disc)\n","    assert disc(x).shape == (N,1,1,1)\n","\n","test()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lB0nxKv1yAC-"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0/5] Batch 0/1583                   Loss D: 0.6892, loss G: 0.7877\n","Epoch [0/5] Batch 100/1583                   Loss D: 0.0164, loss G: 4.0581\n","Epoch [0/5] Batch 200/1583                   Loss D: 0.0069, loss G: 4.9025\n"]}],"source":["\"\"\"\n","Training of DCGAN network on MNIST dataset with Discriminator\n","and Generator imported from models.py\n","\n","Programmed by Aladdin Persson \u003caladdin.persson at hotmail dot com\u003e\n","* 2020-11-01: Initial coding\n","* 2022-12-20: Small revision of code, checked that it works with latest PyTorch version\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","#from models import Discriminator, Generator, initialize_weights\n","\n","# Hyperparameters etc.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n","BATCH_SIZE = 128\n","IMAGE_SIZE = 64\n","CHANNELS_IMG = 3\n","NOISE_DIM = 100\n","NUM_EPOCHS = 5\n","FEATURES_DISC = 64\n","FEATURES_GEN = 64\n","\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize(IMAGE_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n","        ),\n","    ]\n",")\n","\n","# If you train on MNIST, remember to set channels_img to 1\n","# dataset = datasets.MNIST(\n","#     root=\"dataset/\", train=True, transform=transforms, download=True\n","# )\n","\n","# comment mnist above and uncomment below if train on CelebA\n","dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/PyTorch/GANS/celeb_dataset\", transform=transforms)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n","disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n","initialize_weights(gen)\n","initialize_weights(disc)\n","\n","opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()\n","\n","fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n","writer_real = SummaryWriter(f\"logs/real\")\n","writer_fake = SummaryWriter(f\"logs/fake\")\n","step = 0\n","\n","gen.train()\n","disc.train()\n","\n","for epoch in range(NUM_EPOCHS):\n","    # Target labels not needed! \u003c3 unsupervised\n","    for batch_idx, (real, _) in enumerate(dataloader):\n","        real = real.to(device)\n","        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n","        fake = gen(noise)\n","\n","        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n","        disc_real = disc(real).reshape(-1)\n","        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n","        disc_fake = disc(fake.detach()).reshape(-1)\n","        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n","        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n","        disc.zero_grad()\n","        loss_disc.backward()\n","        opt_disc.step()\n","\n","        ### Train Generator: min log(1 - D(G(z))) \u003c-\u003e max log(D(G(z))\n","        output = disc(fake).reshape(-1)\n","        loss_gen = criterion(output, torch.ones_like(output))\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        # Print losses occasionally and print to tensorboard\n","        if batch_idx % 100 == 0:\n","            print(\n","                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n","                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n","            )\n","\n","            with torch.no_grad():\n","                fake = gen(fixed_noise)\n","                # take out (up to) 32 examples\n","                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n","                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n","\n","                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n","                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n","\n","            step += 1"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN85+uC9qHZrsYFeLGI2MDk","gpuType":"T4","mount_file_id":"1lvnWDud0znnUHXDAr6JAApSUAUm_a8uv","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}